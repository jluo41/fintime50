{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Spider\n",
    "\n",
    "\n",
    "In this section, we will write a spider,\n",
    "\n",
    "and run it in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/floyd/Projects/scrapy/pubcrawler/tutorial/6 Run Spider'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the directory to the spider folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/floyd/Projects/scrapy/pubcrawler/tutorial/6 Run Spider/fintime50/fintime50/spiders\n"
     ]
    }
   ],
   "source": [
    "cd /Users/floyd/Projects/scrapy/pubcrawler/tutorial/6\\ Run\\ Spider/fintime50/fintime50/spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `scrapy` command to generate the spider.\n",
    "\n",
    "In spiders directory.\n",
    "\n",
    "```bash\n",
    "scrapy genspider dss www.journals.elsevier.com\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`scrapy`: the command\n",
    "\n",
    "`genspider`: option\n",
    "\n",
    "`dss`: the spider's name\n",
    "\n",
    "`www.journals.elsevier.com`: the domain where you want to scrape.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](genspider.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find a `dss.py` file is generated.\n",
    "\n",
    "Open it and you will find:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class DssSpider(scrapy.Spider):\n",
    "    name = \"dss\"\n",
    "    allowed_domains = [\"www.journals.elsevier.com\"]\n",
    "    start_urls = (\n",
    "        'http://www.www.journals.elsevier.com/',\n",
    "    )\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "```\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file only define a class `DssSpider`, which inherit from `scrapy.Spider`.\n",
    "\n",
    "**Its fields and methods are:**\n",
    "\n",
    "`name`: string, `\"dss\"` is from the command line.\n",
    "\n",
    "`allowed_domians`: list, the domains this spider will scape in.\n",
    "\n",
    "`start_urls`: tuple, when you run the spider, scrapy will request via these URLs, and get `responses`. The `responses` will be conveyed to its method parse, by default.\n",
    "\n",
    "`parse()`: here, this method take `response` as its parameter. \n",
    "\n",
    "**The `response` comes from the `start_urls`. This is very import thing you need to know.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first spider `DssSpider` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In spider, these methods are used.\n",
    "\n",
    "`cleanhtml()`\n",
    "\n",
    "`TakeFirst()`\n",
    "\n",
    "`ItemLoader`\n",
    "\n",
    "\n",
    "In the section `ItemLoader`, these items are illustrated in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scrapy import Item, Field\n",
    "class SourceItem(Item):\n",
    "    publication_title = Field()\n",
    "    chief_editor = Field()\n",
    "    issn = Field()\n",
    "    description = Field()\n",
    "    home_url = Field()\n",
    "    coverimage = Field()\n",
    "    title = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 https://www.journals.elsevier.com/decision-support-systems/>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "\n",
    "# we need headers to disguise our bot as a browser\n",
    "\n",
    "headers = {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36\",\n",
    "    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4,zh-TW;q=0.2\",\n",
    "}\n",
    "\n",
    "\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "r = requests.get('http://www.journals.elsevier.com/decision-support-systems/', \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scrapy.loader.ItemLoader"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialization with Item and response\n",
    "# Item, (here is SourceItem()), it the container ItemLoader uses\n",
    "# response, is the raw material ItemLoader to exploit\n",
    "l = ItemLoader(item = SourceItem(), response = response)\n",
    "type(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here are the xpaths for the items\n",
    "issn_xpath = '//*[@class=\"issn keyword\"]/span/text()'\n",
    "chief_editor_xpath = '//*[@id=\"Title\"]//span[@class=\"nowrap\"]/text()'\n",
    "title_xpath = '//*[@id=\"Title\"]//h1[@itemprop=\"name\"]/text()'\n",
    "description_xpath = '//*[@class=\"publication-description\"]//p'\n",
    "coverimage_xpath = '//*[@id=\"Title\"]//img[@class=\"cover-img\"]/@src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chief_editor': 'James R. Marsden',\n",
       " 'coverimage': 'https://www.elsevier.com/__data/cover_img/505540.gif',\n",
       " 'description': 'The common thread of articles published in Decision Support '\n",
       "                'Systems is their relevance to theoretical and technical '\n",
       "                'issues in the support of enhanced decision making. The areas '\n",
       "                'addressed may include foundations, functionality, interfaces, '\n",
       "                'implementation, impacts, and evaluation of decision support '\n",
       "                'systems... The common thread of articles published in '\n",
       "                'Decision Support Systems is their relevance to theoretical '\n",
       "                'and technical issues in the support of enhanced decision '\n",
       "                'making. The areas addressed may include foundations, '\n",
       "                'functionality, interfaces, implementation, impacts, and '\n",
       "                'evaluation of decision support systems (DSSs). Manuscripts '\n",
       "                'may draw from diverse methods and methodologies, including '\n",
       "                'those from decision theory, economics, econometrics, '\n",
       "                'statistics, computer supported cooperative work, data base '\n",
       "                'management, linguistics, management science, mathematical '\n",
       "                'modeling, operations management, cognitive science, '\n",
       "                'psychology, user interface management, and others. However, a '\n",
       "                'manuscript focused on direct contributions to any of these '\n",
       "                'related areas should be submitted to an outlet appropriate '\n",
       "                'to  the specific area.   Examples of research topics that '\n",
       "                'would be appropriate for Decision Support Systems include the '\n",
       "                'following: 1. DSS Foundations e.g. principles, concepts, and '\n",
       "                'theories of enhanced decision making; formal languages and '\n",
       "                'research methods enabling improvements in decision making. It '\n",
       "                'is important that theory validation be carefully addressed.  '\n",
       "                '2. DSS Functionality e.g. methods, tools, and techniques for '\n",
       "                'developing thefunctional aspects of enhanced decision making; '\n",
       "                'solver, model, and/or data management in DSSs; rule '\n",
       "                'formulation and management in DSSs; DSS development and use '\n",
       "                'in computer supported cooperative work, negotiation, research '\n",
       "                'and product. 3. DSS Interfaces e.g. methods, tools, and '\n",
       "                'techniques for designing and developing DSS interfaces; '\n",
       "                'development, management, and presentation of knowledge in a '\n",
       "                \"DSS;  coordination of a DSS's interface with its \"\n",
       "                'functionality.  4. DSS Implementation - experiences in DSS '\n",
       "                'development and utilization; DSS management and updating; DSS '\n",
       "                'instruction/training.  A critical consideration must be how '\n",
       "                'specific experiences provide more general implications.   5. '\n",
       "                'DSS Evaluation and Impact e.g. evaluation metrics and '\n",
       "                'processes; DSS impact on decision makers, organizational '\n",
       "                'processes and performance.',\n",
       " 'home_url': 'https://www.journals.elsevier.com/decision-support-systems/',\n",
       " 'issn': '0167-9236',\n",
       " 'publication_title': 'Decision Support Systems'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on this logics, we can get a clean item here.\n",
    "import re\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import Join, TakeFirst\n",
    "\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "    \n",
    "    \n",
    "l = ItemLoader(item = SourceItem(), response = response)\n",
    "l.default_output_processor = TakeFirst()\n",
    "\n",
    "# issn\n",
    "l.add_xpath('issn', issn_xpath)\n",
    "\n",
    "# chief_editor\n",
    "l.add_xpath('chief_editor', chief_editor_xpath)\n",
    "\n",
    "# coverimage\n",
    "l.add_xpath('coverimage', coverimage_xpath)\n",
    "\n",
    "# description\n",
    "# notice Join() and cleanhtml here\n",
    "# refer to the units above.\n",
    "l.add_xpath('description', description_xpath, Join(), cleanhtml)\n",
    "\n",
    "# publication_title\n",
    "publication_title = l.get_xpath(title_xpath)\n",
    "l.add_value('publication_title', publication_title)\n",
    "\n",
    "# home_url\n",
    "l.add_value('home_url', response.url)\n",
    "l.load_item()\n",
    "\n",
    "\n",
    "# relatively cleaner now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dss.py` python file content\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import re\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import Join, TakeFirst\n",
    "from scrapy import Spider\n",
    "from dateutil.parser import parse\n",
    "from fintime50.items import SourceItem\n",
    "\n",
    "\n",
    "class DSSSpider(Spider):\n",
    "    name = 'dss'\n",
    "    start_urls = (\n",
    "            \"http://www.journals.elsevier.com/decision-support-systems/\",\n",
    "            )\n",
    "    base_url = \"http://www.sciencedirect.com/science/journal/01679236/\"\n",
    "\n",
    "\n",
    "    def cleanhtml(self, raw_html):\n",
    "      cleanr = re.compile('<.*?>')\n",
    "      cleantext = re.sub(cleanr, '', raw_html)\n",
    "      return cleantext\n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "        issn_xpath = '//*[@class=\"issn keyword\"]/span/text()'\n",
    "        chief_editor_xpath = '//*[@id=\"Title\"]//span[@class=\"nowrap\"]/text()'\n",
    "        title_xpath = '//*[@id=\"Title\"]//h1[@itemprop=\"name\"]/text()'\n",
    "        description_xpath = '//*[@class=\"publication-description\"]//p'\n",
    "        coverimage_xpath = '//*[@id=\"Title\"]//img[@class=\"cover-img\"]/@src'\n",
    "\n",
    "        l = ItemLoader(item = SourceItem(), response = response)\n",
    "        l.default_output_processor = TakeFirst()\n",
    "        l.add_xpath(\"issn\",issn_xpath)\n",
    "        l.add_xpath('chief_editor', chief_editor_xpath)\n",
    "        l.add_xpath('coverimage', coverimage_xpath)\n",
    "        l.add_xpath('description', description_xpath, Join(), self.cleanhtml)\n",
    "        l.add_value('home_url', response.url)\n",
    "        publication_title = l.get_xpath( title_xpath)\n",
    "        l.add_value('publication_title', publication_title)\n",
    "\n",
    "        yield l.load_item()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to run the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/floyd/Projects/scrapy/pubcrawler/tutorial/6 Run Spider/fintime50/fintime50/spiders\r\n"
     ]
    }
   ],
   "source": [
    "# to see the present working directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/floyd/Projects/scrapy/pubcrawler/tutorial/6 Run Spider/fintime50/fintime50\n"
     ]
    }
   ],
   "source": [
    "cd /Users/floyd/Projects/scrapy/pubcrawler/tutorial/6\\ Run\\ Spider/fintime50/fintime50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "├── __init__.py\r\n",
      "├── __pycache__\r\n",
      "│   ├── __init__.cpython-35.pyc\r\n",
      "│   └── settings.cpython-35.pyc\r\n",
      "├── items.py\r\n",
      "├── pipelines.py\r\n",
      "├── settings.py\r\n",
      "└── spiders\r\n",
      "    ├── __init__.py\r\n",
      "    ├── __pycache__\r\n",
      "    │   ├── __init__.cpython-35.pyc\r\n",
      "    │   └── dss.cpython-35.pyc\r\n",
      "    └── dss.py\r\n",
      "\r\n",
      "3 directories, 10 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/floyd/Projects/scrapy/pubcrawler/tutorial/6 Run Spider/fintime50/fintime50\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-11 20:26:58 [scrapy] INFO: Scrapy 1.1.1 started (bot: fintime50)\n",
      "2017-01-11 20:26:58 [scrapy] INFO: Overridden settings: {'ROBOTSTXT_OBEY': True, 'NEWSPIDER_MODULE': 'fintime50.spiders', 'BOT_NAME': 'fintime50', 'SPIDER_MODULES': ['fintime50.spiders']}\n",
      "2017-01-11 20:26:58 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2017-01-11 20:26:58 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-11 20:26:58 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-11 20:26:58 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-11 20:26:58 [scrapy] INFO: Spider opened\n",
      "2017-01-11 20:26:58 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-11 20:26:58 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2017-01-11 20:27:00 [scrapy] DEBUG: Retrying <GET http://www.www.journals.elsevier.com/robots.txt> (failed 1 times): DNS lookup failed: address 'www.www.journals.elsevier.com' not found: [Errno 8] nodename nor servname provided, or not known.\n",
      "2017-01-11 20:27:00 [scrapy] DEBUG: Retrying <GET http://www.www.journals.elsevier.com/robots.txt> (failed 2 times): DNS lookup failed: address 'www.www.journals.elsevier.com' not found: [Errno 8] nodename nor servname provided, or not known.\n",
      "2017-01-11 20:27:00 [scrapy] DEBUG: Gave up retrying <GET http://www.www.journals.elsevier.com/robots.txt> (failed 3 times): DNS lookup failed: address 'www.www.journals.elsevier.com' not found: [Errno 8] nodename nor servname provided, or not known.\n",
      "2017-01-11 20:27:00 [scrapy] ERROR: Error downloading <GET http://www.www.journals.elsevier.com/robots.txt>: DNS lookup failed: address 'www.www.journals.elsevier.com' not found: [Errno 8] nodename nor servname provided, or not known.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/floyd/Environments/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py\", line 1258, in _inlineCallbacks\n",
      "    result = result.throwExceptionIntoGenerator(g)\n",
      "  File \"/Users/floyd/Environments/anaconda3/lib/python3.5/site-packages/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\n",
      "    return g.throw(self.type, self.value, self.tb)\n",
      "  File \"/Users/floyd/Environments/anaconda3/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\n",
      "    defer.returnValue((yield download_func(request=request,spider=spider)))\n",
      "twisted.internet.error.DNSLookupError: DNS lookup failed: address 'www.www.journals.elsevier.com' not found: [Errno 8] nodename nor servname provided, or not known.\n",
      "2017-01-11 20:27:00 [scrapy] DEBUG: Retrying <GET http://www.www.journals.elsevier.com/> (failed 1 times): DNS lookup failed: address 'www.www.journals.elsevier.com' not found: [Errno 8] nodename nor servname provided, or not known.\n",
      "2017-01-11 20:27:00 [scrapy] DEBUG: Retrying <GET http://www.www.journals.elsevier.com/> (failed 2 times): DNS lookup failed: address 'www.www.journals.elsevier.com' not found: [Errno 8] nodename nor servname provided, or not known.\n",
      "2017-01-11 20:27:00 [scrapy] DEBUG: Gave up retrying <GET http://www.www.journals.elsevier.com/> (failed 3 times): DNS lookup failed: address 'www.www.journals.elsevier.com' not found: [Errno 8] nodename nor servname provided, or not known.\n",
      "2017-01-11 20:27:00 [scrapy] ERROR: Error downloading <GET http://www.www.journals.elsevier.com/>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/floyd/Environments/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py\", line 1258, in _inlineCallbacks\n",
      "    result = result.throwExceptionIntoGenerator(g)\n",
      "  File \"/Users/floyd/Environments/anaconda3/lib/python3.5/site-packages/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\n",
      "    return g.throw(self.type, self.value, self.tb)\n",
      "  File \"/Users/floyd/Environments/anaconda3/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\n",
      "    defer.returnValue((yield download_func(request=request,spider=spider)))\n",
      "twisted.internet.error.DNSLookupError: DNS lookup failed: address 'www.www.journals.elsevier.com' not found: [Errno 8] nodename nor servname provided, or not known.\n",
      "2017-01-11 20:27:00 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-11 20:27:00 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/exception_count': 6,\n",
      " 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 6,\n",
      " 'downloader/request_bytes': 1392,\n",
      " 'downloader/request_count': 6,\n",
      " 'downloader/request_method_count/GET': 6,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 11, 12, 27, 0, 599537),\n",
      " 'log_count/DEBUG': 7,\n",
      " 'log_count/ERROR': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2017, 1, 11, 12, 26, 58, 704079)}\n",
      "2017-01-11 20:27:00 [scrapy] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl dss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
