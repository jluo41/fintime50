{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 http://www.sciencedirect.com/science/article/pii/S0167923616301580>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need headers to disguise our bot as a browser\n",
    "\n",
    "headers = {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36\",\n",
    "    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4,zh-TW;q=0.2\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "r = requests.get('http://www.sciencedirect.com/science/article/pii/S0167923616301580', \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Elsevier\n",
    "\n",
    "# home items xpath\n",
    "source = dict(issn = '//*[@class=\"issn keyword\"]/span/text()',\n",
    "chief_editor = '//*[@id=\"Title\"]//span[@class=\"nowrap\"]/text()',\n",
    "publication_title = '//*[@id=\"Title\"]//h1[@itemprop=\"name\"]/text()',\n",
    "description = '//*[@class=\"publication-description\"]//p',\n",
    "coverimage = '//*[@id=\"Title\"]//img[@class=\"cover-img\"]/@src')\n",
    "\n",
    "# article url xpath\n",
    "document_url = '//ol[@class=\"articleList results\"]//a[@class=\"cLink artTitle S_C_artTitle \"]/@href'\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# article fields xpaths\n",
    "document = dict(\n",
    "title = '//*[@class=\"svTitle\"]/text()',\n",
    "abstract = '//div[@class=\"abstract svAbstract \"]/p/text()',\n",
    "date = '//*[@class=\"articleDates\"]/dd/text()',\n",
    "submission_path = '//*[@class=\"volIssue\"]/a/text()',\n",
    "dp = '//*[@class=\"volIssue\"]/text()')\n",
    "\n",
    "# keyword field xpaths\n",
    "keyword = '//*[@class=\"svKeywords\"]/span/text()'\n",
    "\n",
    "# author fields xpaths\n",
    "\n",
    "# within an author selector\n",
    "\n",
    "# version 1:\n",
    "author = dict(\n",
    "auth = '//ul[@class=\"authorGroup noCollab svAuthor\"]/li',\n",
    "fn = 'a[@class=\"authorName svAuthor\"]/@data-fn',\n",
    "ln = 'a[@class=\"authorName svAuthor\"]/@data-ln',\n",
    "email = 'a[@class=\"auth_mail\"]/@href',\n",
    "fid = 'a[@class=\"intra_ref auth_aff\"]/@id',\n",
    "address = '//*[@id=\"%s\"]/span/text()',\n",
    "href = 'span/a[@class=\"authorVitaeLink\"]/@href',\n",
    "vitae = '//p[@id=\"%s\"]/text()',\n",
    "avatar = '//div[@id=\"%shidden\"]//img/@src')\n",
    "\n",
    "\n",
    "\n",
    "# ----------------\n",
    "\n",
    "\n",
    "document0 = dict(\n",
    "title = '//*[@class=\"article-title\"]/text()',\n",
    "abstract = '//div[@class=\"abstract abstract-type-author\"]/div/text()',\n",
    "date = '//*[@class=\"article-history-dates\"]/text()',\n",
    "submission_path = '//*[@class=\"journal-volume\"]/a/text()',\n",
    "dp = '//*[@class=\"journal-volume\"]/text()')\n",
    "\n",
    "\n",
    "keyword0 = '//*[@class=\"keyword\"]/text()'\n",
    "\n",
    "# author fields xpaths\n",
    "\n",
    "# within an author selector\n",
    "\n",
    "# version 0:\n",
    "author0 = dict(\n",
    "auth = '//*[@class=\"author-group\"]',\n",
    "name = './/a/@data-related-url',\n",
    "address = './/*[@class=\"affiliation__text\"]/text()',\n",
    "href = './/*[@class=\"footnote-ref\"]/@href',\n",
    "vitae = '//*[@id=\"%s\"]/dd/text()',\n",
    "email = './/*[@class=\"author-email\"]/@href')\n",
    "\n",
    "relationship = dict(\n",
    "publication_title = '//*[@id=\"Title\"]//h1[@itemprop=\"name\"]/text()',\n",
    "title = '//*[@class=\"svTitle\"]/text()',\n",
    "title0 = '//*[@class=\"article-title\"]/text()'\n",
    ")\n",
    "\n",
    "\n",
    "Xpath = dict(\n",
    "    source = source,\n",
    "    document = document,\n",
    "    author = author,\n",
    "    keyword = keyword,\n",
    "    document0 = document0,\n",
    "    author0 = author0,\n",
    "    keyword0 = keyword0,\n",
    "    relationship = relationship,\n",
    "    document_url = document_url\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = Xpath['source']\n",
    "document = Xpath['document']\n",
    "keyword = Xpath['keyword']\n",
    "author = Xpath['author']\n",
    "document0 = Xpath['document0']\n",
    "keyword0 = Xpath['keyword0']\n",
    "author0 = Xpath['author0']\n",
    "document_url = Xpath['document_url']\n",
    "relationship = Xpath['relationship']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "from scrapy import Item, Field\n",
    "\n",
    "\n",
    "class DocumentItem(Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    abstract = Field()\n",
    "\n",
    "    publication_date = Field()\n",
    "    submission_date = Field()\n",
    "    online_date = Field()\n",
    "    revision_date = Field()\n",
    "    accepted_date = Field()\n",
    "\n",
    "    title = Field()\n",
    "    coverpage_url = Field()\n",
    "    fpage = Field()\n",
    "    lpage = Field()\n",
    "    pages = Field()\n",
    "    submission_path = Field()\n",
    "\n",
    "    publication_title = Field()\n",
    "\n",
    "\n",
    "class KeywordItem(Item):\n",
    "    keyword = Field()\n",
    "\n",
    "    title = Field()\n",
    "\n",
    "\n",
    "class SourceItem(Item):\n",
    "    publication_title = Field()\n",
    "    chief_editor = Field()\n",
    "    issn = Field()\n",
    "    description = Field()\n",
    "    home_url = Field()\n",
    "    coverimage = Field()\n",
    "\n",
    "    title = Field()\n",
    "\n",
    "class AuthorItem(Item):\n",
    "    institution = Field()\n",
    "    email = Field()\n",
    "    avatar = Field()\n",
    "    vitae = Field()\n",
    "    fname = Field()\n",
    "    lname = Field()\n",
    "    address = Field()\n",
    "\n",
    "    title = Field()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import Join, TakeFirst\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def getdate(dates):\n",
    "    d = {}\n",
    "    d['submission_date'] = None\n",
    "    d['revision_date'] = None\n",
    "    d['accepted_date'] = None\n",
    "    d['online_date'] = None\n",
    "    for date in dates:\n",
    "        if 'Received' in date:\n",
    "            d['submission_date'] = parse(date.split('Received ')[-1])\n",
    "        elif 'Revised' in date:\n",
    "            d['revision_date'] = parse(date.split('Revised ')[-1])\n",
    "        elif 'Accepted' in date:\n",
    "            d['accepted_date'] = parse(date.split('Accepted ')[-1])\n",
    "        elif 'Available online' in date:\n",
    "            d['online_date'] = parse(date.split('Available online ')[-1])\n",
    "    return d\n",
    "\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "def load_source(response, source):\n",
    "    l = ItemLoader(item = SourceItem(), response = response)\n",
    "    l.default_output_processor = TakeFirst()\n",
    "    l.add_xpath(\"issn\",source['issn'])\n",
    "    l.add_xpath('chief_editor', source['chief_editor'])\n",
    "    l.add_xpath('publication_title', source['publication_title'])\n",
    "    l.add_xpath('coverimage', source['coverimage'])\n",
    "    l.add_xpath('description', source['description'], Join(), cleanhtml)\n",
    "    l.add_value('home_url', response.url)\n",
    "    publication_title = l.get_xpath(source['publication_title'])\n",
    "    return l\n",
    "\n",
    "\n",
    "\n",
    "def load_document(response, document):\n",
    "    l = ItemLoader(item = DocumentItem(), response = response)\n",
    "    l.default_output_processor = TakeFirst()\n",
    "    l.add_value('coverpage_url', response.url)\n",
    "    l.add_xpath('abstract', document['abstract'])\n",
    "    l.add_xpath('title', document['title'])\n",
    "    l.add_xpath('submission_path', document['submission_path'])\n",
    "\n",
    "    # handle dates\n",
    "    dates = [i for i in l.get_xpath(document['date'])[0].split(', ')]\n",
    "    d = getdate(dates)\n",
    "    l.add_value('submission_date',d['submission_date'])\n",
    "    l.add_value('revision_date',d['revision_date'])\n",
    "    l.add_value('accepted_date', d['accepted_date'])\n",
    "    l.add_value('online_date', d['online_date'])\n",
    "\n",
    "    date_page = l.get_xpath(document['dp'])[0].split(', ')\n",
    "    try:\n",
    "        l.add_value('publication_date', parse(date_page[-2]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # handle pages\n",
    "    try:\n",
    "        pages = date_page[-1].split()[-1]\n",
    "        if '–' in pages:\n",
    "            fp = int(pages.split('–')[0])\n",
    "            lp = int(pages.split('–')[1])\n",
    "        elif '-' in pages:\n",
    "            fp = int(pages.split('-')[0])\n",
    "            lp = int(pages.split('-')[1])\n",
    "        l.add_value('fpage', fp)\n",
    "        l.add_value('lpage', lp)\n",
    "        l.add_value('pages', lp-fp+1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # mark it down, with source's publication_title\n",
    "    return l\n",
    "\n",
    "\n",
    "\n",
    "def load_keyword(response, keyword):\n",
    "    for k in response.xpath(keyword).extract():\n",
    "        k1 = k.split(';')[0]\n",
    "        l = ItemLoader(item = KeywordItem(), response = response)\n",
    "        l.default_output_processor = TakeFirst()\n",
    "        l.add_value('keyword', k1)\n",
    "        yield l\n",
    "\n",
    "\n",
    "\n",
    "def load_author(response,author):\n",
    "    auths = response.xpath(author['auth'])\n",
    "    for auth in auths:\n",
    "        l = ItemLoader(item = AuthorItem(), response = response)\n",
    "        l.default_output_processor = TakeFirst()\n",
    "\n",
    "        # author's first name and last name\n",
    "        fn = auth.xpath(author['fn']).extract()[0]\n",
    "        ln = auth.xpath(author['ln']).extract()[0]\n",
    "        l.add_value('fname', fn)\n",
    "        l.add_value('lname', ln)\n",
    "\n",
    "        # author's email\n",
    "        try:\n",
    "            email = auth.xpath(author['email']).extract()[0][7:]\n",
    "            l.add_value('email', email)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # author's address and institution\n",
    "        try:\n",
    "            fid = auth.xpath(author['fid']).extract()[0][1:]\n",
    "            address = l.get_xpath(author['address'] %fid)\n",
    "\n",
    "            for i in address[0].split(', '):\n",
    "                if 'niversity' in i:\n",
    "                    institution = i\n",
    "                    break\n",
    "            l.add_value('address', address)\n",
    "            l.add_value('institution', institution)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # author's vitae\n",
    "        try:\n",
    "            href = auth.xpath(author['href']).extract()[0][1:]\n",
    "            vitae = response.xpath(author['vitae'] %href).extract()[0]\n",
    "            l.add_value('vitae', fn+' '+ln+vitae)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # author's avatar\n",
    "        try:\n",
    "            href = auth.xpath(author['href']).extract()[0][1:]\n",
    "            avatar = response.xpath(author['avatar'] %href).extract()[0]\n",
    "            l.add_value('avatar', avatar)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        yield l\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_author0(response, author):\n",
    "    auths = response.xpath(author['auth'])\n",
    "    for auth in auths:\n",
    "        l = ItemLoader(item = AuthorItem(), response = response)\n",
    "        l.default_output_processor = TakeFirst()\n",
    "\n",
    "        # add author's fname and lname\n",
    "        name = auth.xpath(author['name']).extract()[0].split('&')[-2:]\n",
    "        fn = name[-1].split('first-name=')[-1]\n",
    "        ln = name[0].split('last-name=')[-1]\n",
    "\n",
    "        l.add_value('fname', fn)\n",
    "        l.add_value('lname', ln)\n",
    "\n",
    "        # add author's email\n",
    "        try:\n",
    "            email = auth.xpath(author['email']).extract()[0][7:]\n",
    "            l.add_value('email', email)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # add author's institution and address\n",
    "        try:\n",
    "            address = auth.xpath(author['address']).extract()\n",
    "            for i in address[0].split(', '):\n",
    "                # elif i in univelist:# institution = i# break\n",
    "                if \"niversity\" in i:\n",
    "                    institution = i\n",
    "                    break\n",
    "            l.add_value('address', address)\n",
    "            l.add_value('institution', institution)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # add author's vitae\n",
    "        try:\n",
    "            href = auth.xpath(author['href']).extract()\n",
    "            vitae = response.xpath(author['vitae'] %href[0][1:]).extract()[0]\n",
    "            l.add_value('vitae', fn+ ' ' +ln+vitae)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        yield l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_document(response):\n",
    "    if len(response.xpath(author['auth'])) != 0:\n",
    "        l = load_document(response, document)\n",
    "        l.add_value('publication_title', 'Decision Support System')\n",
    "        yield l.load_item()\n",
    "        title = response.xpath(relationship['title']).extract()[0]\n",
    "        for l in load_keyword(response, keyword):\n",
    "            l.add_value('title', title)\n",
    "            yield l.load_item()\n",
    "        for l in load_author(response, author):\n",
    "            l.add_value('title', title)\n",
    "            yield l.load_item()\n",
    "    elif len(response.xpath(author0['auth'])) != 0:\n",
    "        l = load_document(response, document0)\n",
    "        l.add_value('publication_title', response.meta['publication_title'])\n",
    "        yield l.load_item()\n",
    "        title = response.xpath(relationship['title0']).extract()[0]\n",
    "        for l in load_keyword(response, keyword0):\n",
    "            l.add_value('title', title)\n",
    "            yield l.load_item()\n",
    "        for l in load_author0(response, author0):\n",
    "            l.add_value('title', title)\n",
    "            yield l.load_item()\n",
    "    else:\n",
    "        print(\" $ No Authors $   \", response.url, \" <-----   LOOK HERE! ~\\('o ')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': \"This study explores the role of norms in employees' compliance \"\n",
      "             'with an organizational information security policy (ISP). '\n",
      "             'Drawing upon norm activation theory, social norms theory, and '\n",
      "             'ethical climate literature, we propose a model to examine how '\n",
      "             'ISP-related personal norms are developed and then activated to '\n",
      "             \"affect employees' ISP compliance behavior. We collected our data \"\n",
      "             'through Amazon Mechanical Turk for hypothesis testing. The '\n",
      "             'results show that ISP-related personal norms lead to ISP '\n",
      "             'compliance behavior, and the effect is strengthened by '\n",
      "             'ISP-related ascription of personal responsibility. Social norms '\n",
      "             'related to ISP (including injunctive and subjective norms), '\n",
      "             'awareness of consequences, and ascription of personal '\n",
      "             'responsibility shape personal norms. Social norms related to ISP '\n",
      "             'are the product of principle ethical climate in an organization.',\n",
      " 'accepted_date': datetime.datetime(2016, 9, 13, 0, 0),\n",
      " 'coverpage_url': 'http://www.sciencedirect.com/science/article/pii/S0167923616301580',\n",
      " 'online_date': datetime.datetime(2016, 9, 22, 0, 0),\n",
      " 'publication_date': datetime.datetime(2016, 12, 28, 0, 0),\n",
      " 'publication_title': 'Decision Support System',\n",
      " 'revision_date': datetime.datetime(2016, 7, 29, 0, 0),\n",
      " 'submission_date': datetime.datetime(2015, 8, 29, 0, 0),\n",
      " 'submission_path': 'Volume 92',\n",
      " 'title': \"Employees' information security policy compliance: A norm \"\n",
      "          'activation perspective'}\n",
      "{'keyword': 'Information security compliance',\n",
      " 'title': \"Employees' information security policy compliance: A norm \"\n",
      "          'activation perspective'}\n",
      "{'keyword': 'Personal norms',\n",
      " 'title': \"Employees' information security policy compliance: A norm \"\n",
      "          'activation perspective'}\n",
      "{'keyword': 'Social norms',\n",
      " 'title': \"Employees' information security policy compliance: A norm \"\n",
      "          'activation perspective'}\n",
      "{'keyword': 'Principle ethical climate',\n",
      " 'title': \"Employees' information security policy compliance: A norm \"\n",
      "          'activation perspective'}\n",
      "{'email': 'adel@uta.edu',\n",
      " 'fname': 'Adel',\n",
      " 'lname': 'Yazdanmehr',\n",
      " 'title': \"Employees' information security policy compliance: A norm \"\n",
      "          'activation perspective'}\n",
      "{'email': 'jwang@uta.edu',\n",
      " 'fname': 'Jingguo',\n",
      " 'lname': 'Wang',\n",
      " 'title': \"Employees' information security policy compliance: A norm \"\n",
      "          'activation perspective'}\n"
     ]
    }
   ],
   "source": [
    "for i in parse_document(response):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
